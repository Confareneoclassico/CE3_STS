{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215aa03b-3931-49b4-86e4-c504e1934ca2",
   "metadata": {},
   "source": [
    "# Statistical Analysis II - Practicum 2 (Week 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e08e7-1fa2-45db-9888-a695af903c82",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc793b5-00af-49c8-8faa-5b4a442afe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d62bb1-2a00-461c-b06b-ced3247407aa",
   "metadata": {},
   "source": [
    "### Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ac0a1-2c43-402c-a7ef-db0946929c7a",
   "metadata": {},
   "source": [
    "Resources from [url1](https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/), [url2](https://pub.towardsai.net/principal-component-analysis-pca-with-python-examples-tutorial-67a917bae9aa), [url3](https://towardsdatascience.com/from-covariance-matrix-to-principle-component-analysis-d101cc50934e), [url4](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c8a36-57b3-4466-b4cf-e912a31f8366",
   "metadata": {},
   "source": [
    "**Principle component analysis (PCA)*** is based on simple linear algebra operations (see more in _Lecture 7_ and following.\n",
    "\n",
    "Among PCA uses: reduction of dimensionality of data (machine learning applications or other sort of problems prone to 'the curse of dimensionality', i.e. when the computational cost scales (super)linearly with the dimensionality (the number of variables at play) of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008c074-fff3-45cf-8aa6-d1a0353d6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(X, Y):\n",
    "    # Calculate the covariance between two random variables.\n",
    "    xbar = np.mean(X)\n",
    "    ybar = np.mean(Y)\n",
    "    covar = 1 / (len(X) - 1) * np.sum( (X - xbar) * (Y - ybar) )\n",
    "    return covar\n",
    "\n",
    "\n",
    "def covariance_matrix(matrix):\n",
    "    # Calculate the variance-covariance matrix.\n",
    "    n_var = len(matrix.T)\n",
    "    C = np.zeros([n_var, n_var])\n",
    "    for ix_1, variable_1 in enumerate(matrix.T):\n",
    "        for ix_2, variable_2 in enumerate(matrix.T):\n",
    "            C[ix_1, ix_2] = covariance(variable_1, variable_2)\n",
    "    return C\n",
    "\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9d236-73f3-450b-b3a7-5ebe8ca44e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a matrix\n",
    "Mat = np.array([[7, 5], [9, 1]])\n",
    "print('Mat: ', Mat)\n",
    "# calculate the mean of each column\n",
    "M = np.mean(Mat.T, axis=1)\n",
    "print('Mean: ', M)\n",
    "# center columns by subtracting column means\n",
    "C = Mat - M\n",
    "print('Centred: ', C)\n",
    "# calculate covariance matrix of centered matrix\n",
    "V = np.cov(C.T)\n",
    "print('Cov: ', V)\n",
    "Cov = covariance_matrix(C)\n",
    "print('Cov_function: ', Cov)\n",
    "# eigendecomposition of covariance matrix\n",
    "values, vectors = np.linalg.eig(V)\n",
    "print('eigenvectors: ', vectors)\n",
    "print('eigenvalues: ', values)\n",
    "# project data\n",
    "P = vectors.T.dot(C.T)\n",
    "print('project data:' , P.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d15fd0-3cb0-4d5d-b6c5-e3e261db8205",
   "metadata": {},
   "source": [
    "Correlation can also visualised well through heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d0720-bbe9-4c79-a09e-528b9a03c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.matshow(V, cmap=plt.cm.rainbow)\n",
    "plt.colorbar(img, ticks = [-8, 0, 8], fraction=0.045) \n",
    "for x in range(V.shape[0]):\n",
    "    for y in range(V.shape[1]):\n",
    "        plt.text(x, y, \"%0.2f\" % V[x,y], size=12, color='black', ha=\"center\", va=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af3b983-3948-4d41-b291-11fe184b6b9f",
   "metadata": {},
   "source": [
    "We can make use of a class directly available from the sklearn library, **PCA**, documentation available from [here](\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc65c2a-c86b-4940-9fb9-9b3798b0c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the PCA instance\n",
    "pca = PCA(2)\n",
    "# fit on data\n",
    "pca.fit(Mat)\n",
    "# access values and vectors\n",
    "print('components:' , pca.components_)\n",
    "print('explained variance: ', pca.explained_variance_)\n",
    "# transform data\n",
    "Tr = pca.transform(Mat)\n",
    "print('transformed components: ', Tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01bc37-a603-4ed7-9173-1b41649489b9",
   "metadata": {},
   "source": [
    "Suppose we have more points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5b0af-fc5a-4767-b2f9-e6cb79cae80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "n_variables = 2\n",
    "mu = 10\n",
    "sigma = 2.5\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.normal(mu, sigma, size=(n_samples, n_variables))\n",
    "\n",
    "# Plotting (Additional plot styling is shown in the Notebook)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 4))\n",
    "ax.scatter(X[:, 0], X[:, 1], alpha=0.85, color='lightgreen', edgecolors='green', s=15, linewidth=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf61e2cd-6276-4349-9188-4b20fa99cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, y) = X.T  # split into two features\n",
    "\n",
    "print(f\"Variance for x   : {covariance(x, x)}\")\n",
    "print(f\"Covariance of x,y: {covariance(x, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2fee16-85cf-4420-b99d-cd58771ec8fe",
   "metadata": {},
   "source": [
    "We would have expected to get the square of the standard deviation, we were close, but the value showed a mismatch due to the limited population.\n",
    "\n",
    "How do things change if one is using actually correlated data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b857c-0aad-43c1-a5dc-eb22d036c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale x and y distribution\n",
    "sx, sy = 0.8, 4\n",
    "Scale = np.array([\n",
    "    [ sx, 0 ], \n",
    "    [ 0,  sy],\n",
    "])\n",
    "\n",
    "# Rotate data\n",
    "theta = 0.35 * np.pi\n",
    "Rot = np.array([\n",
    "    [ np.cos(theta), -np.sin(theta)], \n",
    "    [ np.sin(theta),  np.cos(theta)],\n",
    "])\n",
    "\n",
    "T = Scale.dot(Rot)\n",
    "\n",
    "# Transform data\n",
    "X2 = X.dot(T)\n",
    "X2[:,1] += 4  # lift data a bit to not have negative weights\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 4))\n",
    "ax.scatter(X2[:, 0], X2[:, 1], alpha=0.85, color='lightblue', edgecolors='blue', s=15, linewidth=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060ce87-64be-4346-b5e5-a6ffeeb336de",
   "metadata": {},
   "source": [
    "We can visualise the principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6e007-aa4d-4007-9664-ce37c3a742de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X2)\n",
    "print('pca components: ', pca.components_)\n",
    "\n",
    "print('pca explained variance: ', pca.explained_variance_)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X2[:, 0], X2[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9296e7ec-d584-4ccd-a60e-df36bddf03ab",
   "metadata": {},
   "source": [
    "How does the covariance matrix look like now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9f045-2412-4c34-bab2-f1f74a440759",
   "metadata": {},
   "source": [
    "Let us now consider an example of a larger data sets with more variables (_features_ in the machine learning jaergon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4dce67-92a5-4a4f-9d17-a7ed39708aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "winedata = load_wine()\n",
    "X, y = winedata['data'], winedata['target']\n",
    "\n",
    "print('variables: ', X.shape)\n",
    "print('output variables: ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264c0b7-f397-4e25-b9f7-da458603787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's produce a scatter plot of the relations between the variables\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[:,1], X[:,2], c=y)\n",
    "plt.xlabel(winedata[\"feature_names\"][1])\n",
    "plt.ylabel(winedata[\"feature_names\"][8])\n",
    "plt.title(\"Two particular features of the wine dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e64e5-322e-40c8-a893-99fe9b66758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or even a 3d-plot\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(X[:,2], X[:,6], X[:,7], c=y)\n",
    "ax.set_xlabel(winedata[\"feature_names\"][2])\n",
    "ax.set_ylabel(winedata[\"feature_names\"][6])\n",
    "ax.set_zlabel(winedata[\"feature_names\"][7])\n",
    "ax.set_title(\"Three particular features of the wine dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98b12f-1ae3-495f-8a91-e76142727b6a",
   "metadata": {},
   "source": [
    "We can perform a PCA to try to tell apart the different classes of wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93076345-9663-44b4-9090-4778a531a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "plt.figure(figsize=(8,6))\n",
    "Xt = pca.fit_transform(X)\n",
    "plot = plt.scatter(Xt[:,0], Xt[:,1], c=y)\n",
    "plt.legend(handles=plot.legend_elements()[0], labels=list(winedata['target_names']))\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"First two principal components\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894bc811-d518-40eb-90f7-008b78814c72",
   "metadata": {},
   "source": [
    "They are not so well separated, let's try by normalising the variables (setting $E = 0, \\sigma = 1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d5136-7a53-469b-9dbd-101141ca841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
    "plt.figure(figsize=(8,6))\n",
    "Xt = pipe.fit_transform(X)\n",
    "plot = plt.scatter(Xt[:,0], Xt[:,1], c=y)\n",
    "plt.legend(handles=plot.legend_elements()[0], labels=list(winedata['target_names']))\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"First two principal components after scaling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc60d1-f30c-4756-891d-ff241b60fcee",
   "metadata": {},
   "source": [
    "How many components does the PCA show?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e3c2e-dc91-4b30-b50d-b8b933f20ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31abef3b-755e-434d-9b13-b510b3a2d009",
   "metadata": {},
   "source": [
    "How are the pca components related?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381c031-c8e4-49fc-8637-0f384fdfa718",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov = pca.get_covariance()\n",
    "\n",
    "img = plt.matshow(Cov, cmap=plt.cm.rainbow)\n",
    "plt.colorbar(img, ticks = [-1, 0, 1], fraction=0.045) \n",
    "for x in range(Cov.shape[0]):\n",
    "    for y in range(Cov.shape[1]):\n",
    "        plt.text(x, y, \"%0.2f\" % Cov[x,y], size=6, color='black', ha=\"center\", va=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1765fdae-7e75-4f7f-b042-e551c0e29c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "What fractions of the variance do these components explain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f874dd3-72b5-48b7-8fd1-0be66cf453fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.axhline(y=0.9, color='r', linestyle=':',alpha=.5)\n",
    "plt.axhline(y=0.95, color='g', linestyle=':',alpha=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d7927-9a62-482a-bade-37844cf4949c",
   "metadata": {},
   "source": [
    "PCA can be used for dimensionality reduction (say, one wants _n_ components that apportion at least 90, 95, or 99% of the variance). In this way one can reduce the dimensionality of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9568629b-53c0-4b0e-9404-438df312d60e",
   "metadata": {},
   "source": [
    "One caveat of PCA is that it may be difficult communicating the results of the analysis in terms of understanding the significant of the principal components as combination of the base variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
